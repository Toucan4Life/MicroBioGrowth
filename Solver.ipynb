{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "#Load file\n",
    "dataframe = pd.read_csv(\"data.csv\", delimiter=\";\")\n",
    "\n",
    "print(f\"We got {len(dataframe)} rows and {len(dataframe.columns)} columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data\n",
    "features = dataframe.iloc[:, [2,4,6,8,10,12,14,16,18,20,21,24,26,28,30,32,33,35,36,37,38,41,42]]\n",
    "target = dataframe.iloc[:, [43]]\n",
    "\n",
    "features.loc[features['CRP    '].str.startswith('<'),'CRP    ']=0\n",
    "#Treat string data as NaN\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "for col in features:\n",
    "    features[col]=pd.to_numeric(features[col], errors='coerce') \n",
    "\n",
    "#Replace NaN by mean value\n",
    "imp = SimpleImputer(missing_values=pd.NA, strategy='median')\n",
    "features=pd.DataFrame(imp.fit_transform(features), columns=features.columns)\n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem space has >10 dimensions, we fall into https://en.wikipedia.org/wiki/Curse_of_dimensionality. \n",
    "#Let's find the most relevant dimension using PCA\n",
    "\n",
    "#We first need to normalize the data\n",
    "scaledFeatures = pd.DataFrame(preprocessing.scale(features),columns = features.columns)\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "principalComponents = pca.fit_transform(scaledFeatures)\n",
    "\n",
    "print (f\"percentage of precision with the number of components :\\n {pca.explained_variance_ratio_.cumsum()}\")\n",
    "\n",
    "#relationship from data to component\n",
    "#print(pd.DataFrame(pca.components_,columns=x.columns))\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "finalDf = pd.concat([principalDf, target], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the first 2 principals components\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot() \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "cond = finalDf.iloc[:,-1:] == 0\n",
    "cond = cond.squeeze()\n",
    "subset_a = finalDf[cond.squeeze()]\n",
    "subset_b = finalDf[~cond]\n",
    "plt.scatter(subset_a.iloc[:,1], subset_a.iloc[:,2], s=60, c='b', label='Sterile')\n",
    "plt.scatter(subset_b.iloc[:,1], subset_b.iloc[:,2], s=60, c='r', label='Infected') \n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to see the matrix of correlation\n",
    "import seaborn as sns\n",
    "scaledFeatures2 = scaledFeatures.copy()\n",
    "scaledFeatures2['target'] = target\n",
    "cor = scaledFeatures2.corr()\n",
    "\t\t     \n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(cor,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExhaustiveFeatureSelector, but take an eternity\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "# efs=ExhaustiveFeatureSelector(RandomForestClassifier(),min_features=2, max_features=4,scoring = 'roc_auc',print_progress=True)\n",
    "# efs = efs.fit(scaledFeatures, target.values.ravel())\n",
    "# print('Best accuracy score: %.2f' % efs.best_score_)\n",
    "# print('Best subset (indices):', efs.best_idx_)\n",
    "# print('Best subset (corresponding names):', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "v_Treshold = VarianceThreshold(threshold=0.1)\n",
    "v_Treshold.fit(features)\n",
    "v_Treshold.get_support()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "ffs = SequentialFeatureSelector(knn, n_features_to_select='auto',tol=0.000001)\n",
    "ffs.fit(scaledFeatures.values,target.values.ravel())\n",
    "ffs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to classify anyway, using https://medium.com/thrive-in-ai/classification-algorithms-in-python-5f58a7a27b88\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(features, target, test_size=0.2)\n",
    "lr_clf = LogisticRegression().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = RandomForestClassifier().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = GaussianNB().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Nearest Neighbours Classification\n",
    "from sklearn import neighbors\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = neighbors.KNeighborsClassifier().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SVM Classifier\n",
    "from sklearn import svm\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = svm.SVC(kernel = 'linear').fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using gradient boosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = GradientBoostingClassifier().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(scaledFeatures, target, test_size=0.2)\n",
    "lr_clf = tree.DecisionTreeClassifier().fit(preprocessing.scale(X_train),Y_train.values.ravel())\n",
    "lr_clf.score(preprocessing.scale(X_test),Y_test)\n",
    "tree.plot_tree(lr_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
